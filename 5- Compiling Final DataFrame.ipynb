{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"finaldataframe.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Notebook 5- Compiling Final DataFrame\n",
    "\n",
    "### _ Merging Seeking Alpha Analyses with Historical Stock Data_\n",
    "\n",
    "---\n",
    "\n",
    "### Notebook Summary\n",
    "- The method for merging historical stock data with long and short idea analyses from Seeking Alpha will be utilizing Pandas \"datetime\" functionality to:\n",
    "     - Create weekly offsets for one year (52 offsets) that will represent holding periods\n",
    "     - Using the time of article posting as our starting point (aka offset 0 = time of article posting), engineering new columns by applying offsets to \"Time\"\n",
    "     - Performing a merge for each week, which will append the appropriate stock data to the corresponding date and ticker \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Final Stocks & Ideas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = pd.read_csv('final_stocks.csv')  # Exported from Notebook 3\n",
    "ideas = pd.read_csv('final_ideas_df.csv') # Exported from Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Let's Check It Out!\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ideas = ideas.rename(columns={'Time':'Date'}) #DROP THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### & make sure there are no missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ideas.isnull().sum() - No missing values\n",
    "# stocks.isnull().sum() - No missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### & check out the datatypes we are working with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ideas.dtypes\n",
    "# stocks.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = stocks[['Date','Close','Tickers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ideas = ideas[['Authors','Link','Tickers','Date','Title','Strategy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Datatype Conversion \n",
    "_In order to properly interpret, analyze and model stock recommendations with historical stock pricing timeseries functionality, we must convert datatypes in order to merge the dataframes._\n",
    "\n",
    "\n",
    "\n",
    "`Date` : Timestamp for each observation/date\n",
    "- We will be relying heavily on Pandas DateTime for timeseries functionality \n",
    "- To do so, different holding periods need to be created with 'Date' as their starting point\n",
    "- Historical stock data will be merged with these holding periods to investigate how accurate recommendations are when the dust settles \n",
    "- Creating holding periods aka `offsets` in  `weeklyincrements` \n",
    "\n",
    "---\n",
    "### Converting columns to Pandas datetime objects\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2017-12-31 21:16:00\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "0   2017-01-03\n",
      "Name: Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "ideas['Date'] = pd.to_datetime(ideas['Date'])\n",
    "stocks['Date'] = pd.to_datetime(stocks['Date'])\n",
    "\n",
    "# Check progress\n",
    "\n",
    "print(ideas['Date'].head(1))\n",
    "print(stocks['Date'].head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Creating weekly offsets for 1 year\n",
    "----\n",
    "_Note : The largest increment timedelta can hold is days_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = timedelta(days=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling list of weeks in terms of days (multiples of 7)\n",
    "#\n",
    "weeks = list(range(7,365,7)) \n",
    "#\n",
    "# Compiling list of timedeltas for same range\n",
    "offsets = list(pd.timedelta_range(start, periods=52, freq= '7D'))\n",
    "#\n",
    "# Iterating through both lists, adding each offset to time of article posting\n",
    "for weeks_value, offsets_value in zip(weeks, offsets):\n",
    "    ideas[weeks_value] = ideas['Date'] + offsets_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Converting new columns to datetime objects\n",
    "----\n",
    "_Note : Exclude categorical data and merge back after datetime transformation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Grabbing categorical columns\n",
    "categorical = ideas[['Authors','Link','Tickers','Strategy','Title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating lambda function to map datetime transformation to datetime columns\n",
    "to_datetime = lambda x: pd.to_datetime(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dropping categorical columns so that applymap() can run\n",
    "ideas.drop(['Authors', 'Link', 'Tickers', 'Title', 'Strategy'],axis=1,inplace=True)\n",
    "\n",
    "# Applying functionn\n",
    "ideas = ideas.applymap(to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Extracting week number for Ideas & Stocks\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creatinng lambda function to map week accessor to datetime columns\n",
    "to_week = lambda x: x.dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying function to Ideas DataFrame\n",
    "for column in ideas:\n",
    "    ideas[column] = ideas[column].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying function to Stocks DataFrame \n",
    "stocks['Date'] = stocks['Date'].dt.week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Joining categorical data back with Ideas DataFrame\n",
    "ideas = categorical.join(ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Merging Ideas & Stocks DataFrames\n",
    "----\n",
    "- Left merge\n",
    "- Align column names\n",
    "- 52 merges represent 52 offsets created for 52 weeks out of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks = stocks.rename(columns={'Close':'Opening Price'})\n",
    "opening = pd.merge(stocks,ideas,on=['Date','Tickers'])\n",
    "opening.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week1 = opening.rename(columns={7:'7'})\n",
    "stocks = stocks.rename(columns={'Opening Price':'Week1','Date':'7'})\n",
    "week1 = pd.merge(stocks,week1,on=['7','Tickers'])\n",
    "week1.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week2 = week1.rename(columns={14:'14'})\n",
    "stocks = stocks.rename(columns={'Week1':'Week2','7':'14'})\n",
    "week2 = pd.merge(stocks,week2,on=['14','Tickers'])\n",
    "week2.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week3 = week2.rename(columns={21:'21'})\n",
    "stocks = stocks.rename(columns={'Week2':'Week3','14':'21'})\n",
    "week3 = pd.merge(stocks,week3,on=['21','Tickers'])\n",
    "week3.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week4 = week3.rename(columns={28:'28'})\n",
    "stocks = stocks.rename(columns={'Week3':'Week4','21':'28'})\n",
    "week4 = pd.merge(stocks,week4,on=['28','Tickers'])\n",
    "week4.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week5 = week4.rename(columns={35:'35'})\n",
    "stocks = stocks.rename(columns={'Week4':'Week5','28':'35'})\n",
    "week5 = pd.merge(stocks,week5,on=['35','Tickers'])\n",
    "week5.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week6 = week5.rename(columns={42:'42'})\n",
    "stocks = stocks.rename(columns={'Week5':'Week6','35':'42'})\n",
    "week6 = pd.merge(stocks,week6,on=['42','Tickers'])\n",
    "week6.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week7 = week6.rename(columns={49:'49'})\n",
    "stocks = stocks.rename(columns={'Week6':'Week7','42':'49'})\n",
    "week7 = pd.merge(stocks,week7,on=['49','Tickers'])\n",
    "week7.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week8 = week7.rename(columns={56:'56'})\n",
    "stocks = stocks.rename(columns={'Week7':'Week8','49':'56'})\n",
    "week8 = pd.merge(stocks,week8,on=['56','Tickers'])\n",
    "week8.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week9 = week8.rename(columns={63:'63'})\n",
    "stocks = stocks.rename(columns={'Week8':'Week9','56':'63'})\n",
    "week9 = pd.merge(stocks,week9,on=['63','Tickers'])\n",
    "week9.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week10 = week9.rename(columns={70:'70'})\n",
    "stocks = stocks.rename(columns={'Week9':'Week10','63':'70'})\n",
    "week10 = pd.merge(stocks,week10,on=['70','Tickers'])\n",
    "week10.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week11 = week10.rename(columns={77:'77'})\n",
    "stocks = stocks.rename(columns={'Week10':'Week11','70':'77'})\n",
    "week11 = pd.merge(stocks,week11,on=['77','Tickers'])\n",
    "week11.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week12 = week11.rename(columns={84:'84'})\n",
    "stocks = stocks.rename(columns={'Week11':'Week12','77':'84'})\n",
    "week12 = pd.merge(stocks,week12,on=['84','Tickers'])\n",
    "week12.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week13 = week12.rename(columns={91:'91'})\n",
    "stocks = stocks.rename(columns={'Week12':'Week13','84':'91'})\n",
    "week13 = pd.merge(stocks,week13,on=['91','Tickers'])\n",
    "week13.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week14 = week13.rename(columns={98:'98'})\n",
    "stocks = stocks.rename(columns={'Week13':'Week14','91':'98'})\n",
    "week14 = pd.merge(stocks,week14,on=['98','Tickers'])\n",
    "week14.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week15 = week14.rename(columns={105:'105'})\n",
    "stocks = stocks.rename(columns={'Week14':'Week15','98':'105'})\n",
    "week15 = pd.merge(stocks,week15,on=['105','Tickers'])\n",
    "week15.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week16 = week15.rename(columns={112:'112'})\n",
    "stocks = stocks.rename(columns={'Week15':'Week16','105':'112'})\n",
    "week16 = pd.merge(stocks,week16,on=['112','Tickers'])\n",
    "week16.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week17 = week16.rename(columns={119:'119'})\n",
    "stocks = stocks.rename(columns={'Week16':'Week17','112':'119'})\n",
    "week17 = pd.merge(stocks,week17,on=['119','Tickers'])\n",
    "week17.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week18 = week17.rename(columns={126:'126'})\n",
    "stocks = stocks.rename(columns={'Week17':'Week18','119':'126'})\n",
    "week18 = pd.merge(stocks,week18,on=['126','Tickers'])\n",
    "week18.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week19 = week18.rename(columns={133:'133'})\n",
    "stocks = stocks.rename(columns={'Week18':'Week19','126':'133'})\n",
    "week19 = pd.merge(stocks,week19,on=['133','Tickers'])\n",
    "week19.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week20 = week19.rename(columns={140:'140'})\n",
    "stocks = stocks.rename(columns={'Week19':'Week20','133':'140'})\n",
    "week20 = pd.merge(stocks,week20,on=['140','Tickers'])\n",
    "week20.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week21 = week20.rename(columns={147:'147'})\n",
    "stocks = stocks.rename(columns={'Week20':'Week21','140':'147'})\n",
    "week21 = pd.merge(stocks,week21,on=['147','Tickers'])\n",
    "week21.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week22 = week21.rename(columns={154:'154'})\n",
    "stocks = stocks.rename(columns={'Week21':'Week22','147':'154'})\n",
    "week22 = pd.merge(stocks,week22,on=['154','Tickers'])\n",
    "week22.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week23 = week22.rename(columns={161:'161'})\n",
    "stocks = stocks.rename(columns={'Week22':'Week23','154':'161'})\n",
    "week23 = pd.merge(stocks,week23,on=['161','Tickers'])\n",
    "week23.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week24 = week23.rename(columns={168:'168'})\n",
    "stocks = stocks.rename(columns={'Week23':'Week24','161':'168'})\n",
    "week24 = pd.merge(stocks,week24,on=['168','Tickers'])\n",
    "week24.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week25 = week24.rename(columns={175:'175'})\n",
    "stocks = stocks.rename(columns={'Week24':'Week25','168':'175'})\n",
    "week25 = pd.merge(stocks,week25,on=['175','Tickers'])\n",
    "week25.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week26 = week25.rename(columns={182:'182'})\n",
    "stocks = stocks.rename(columns={'Week25':'Week26','175':'182'})\n",
    "week26 = pd.merge(stocks,week26,on=['182','Tickers'])\n",
    "week26.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week27 = week26.rename(columns={189:'189'})\n",
    "stocks = stocks.rename(columns={'Week26':'Week27','182':'189'})\n",
    "week27 = pd.merge(stocks,week27,on=['189','Tickers'])\n",
    "week27.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week28 = week27.rename(columns={196:'196'})\n",
    "stocks = stocks.rename(columns={'Week27':'Week28','189':'196'})\n",
    "week28 = pd.merge(stocks,week28,on=['196','Tickers'])\n",
    "week28.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week29 = week28.rename(columns={203:'203'})\n",
    "stocks = stocks.rename(columns={'Week28':'Week29','196':'203'})\n",
    "week29 = pd.merge(stocks,week29,on=['203','Tickers'])\n",
    "week29.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week30 = week29.rename(columns={210:'210'})\n",
    "stocks = stocks.rename(columns={'Week29':'Week30','203':'210'})\n",
    "week30 = pd.merge(stocks,week30,on=['210','Tickers'])\n",
    "week30.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week31 = week30.rename(columns={217:'217'})\n",
    "stocks = stocks.rename(columns={'Week30':'Week31','210':'217'})\n",
    "week31 = pd.merge(stocks,week31,on=['217','Tickers'])\n",
    "week31.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week32 = week31.rename(columns={224:'224'})\n",
    "stocks = stocks.rename(columns={'Week31':'Week32','217':'224'})\n",
    "week32 = pd.merge(stocks,week32,on=['224','Tickers'])\n",
    "week32.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week33 = week32.rename(columns={231:'231'})\n",
    "stocks = stocks.rename(columns={'Week32':'Week33','224':'231'})\n",
    "week33 = pd.merge(stocks,week33,on=['231','Tickers'])\n",
    "week33.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week34 = week33.rename(columns={238:'238'})\n",
    "stocks = stocks.rename(columns={'Week33':'Week34','231':'238'})\n",
    "week34 = pd.merge(stocks,week34,on=['238','Tickers'])\n",
    "week34.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week35 = week34.rename(columns={245:'245'})\n",
    "stocks = stocks.rename(columns={'Week34':'Week35','238':'245'})\n",
    "week35 = pd.merge(stocks,week35,on=['245','Tickers'])\n",
    "week35.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week36 = week35.rename(columns={252:'252'})\n",
    "stocks = stocks.rename(columns={'Week35':'Week36','245':'252'})\n",
    "week36 = pd.merge(stocks,week36,on=['252','Tickers'])\n",
    "week36.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week37 = week36.rename(columns={259:'259'})\n",
    "stocks = stocks.rename(columns={'Week36':'Week37','252':'259'})\n",
    "week37 = pd.merge(stocks,week37,on=['259','Tickers'])\n",
    "week37.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week38 = week37.rename(columns={266:'266'})\n",
    "stocks = stocks.rename(columns={'Week37':'Week38','259':'266'})\n",
    "week38 = pd.merge(stocks,week38,on=['266','Tickers'])\n",
    "week38.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week39 = week38.rename(columns={273:'273'})\n",
    "stocks = stocks.rename(columns={'Week38':'Week39','266':'273'})\n",
    "week39 = pd.merge(stocks,week39,on=['273','Tickers'])\n",
    "week39.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week40 = week39.rename(columns={280:'280'})\n",
    "stocks = stocks.rename(columns={'Week39':'Week40','273':'280'})\n",
    "week40 = pd.merge(stocks,week40,on=['280','Tickers'])\n",
    "week40.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week41 = week40.rename(columns={287:'287'})\n",
    "stocks = stocks.rename(columns={'Week40':'Week41','280':'287'})\n",
    "week41 = pd.merge(stocks,week41,on=['287','Tickers'])\n",
    "week41.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week42 = week41.rename(columns={294:'294'})\n",
    "stocks = stocks.rename(columns={'Week41':'Week42','287':'294'})\n",
    "week42 = pd.merge(stocks,week42,on=['294','Tickers'])\n",
    "week42.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week43 = week42.rename(columns={301:'301'})\n",
    "stocks = stocks.rename(columns={'Week42':'Week43','294':'301'})\n",
    "week43 = pd.merge(stocks,week43,on=['301','Tickers'])\n",
    "week43.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week44 = week43.rename(columns={308:'308'})\n",
    "stocks = stocks.rename(columns={'Week43':'Week44','301':'308'})\n",
    "week44 = pd.merge(stocks,week44,on=['308','Tickers'])\n",
    "week44.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week45 = week44.rename(columns={315:'315'})\n",
    "stocks = stocks.rename(columns={'Week44':'Week45','308':'315'})\n",
    "week45 = pd.merge(stocks,week45,on=['315','Tickers'])\n",
    "week45.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week46 = week45.rename(columns={322:'322'})\n",
    "stocks = stocks.rename(columns={'Week45':'Week46','315':'322'})\n",
    "week46 = pd.merge(stocks,week46,on=['322','Tickers'])\n",
    "week46.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week47 = week46.rename(columns={329:'329'})\n",
    "stocks = stocks.rename(columns={'Week46':'Week47','322':'329'})\n",
    "week47 = pd.merge(stocks,week47,on=['329','Tickers'])\n",
    "week47.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week48 = week47.rename(columns={336:'336'})\n",
    "stocks = stocks.rename(columns={'Week47':'Week48','329':'336'})\n",
    "week48 = pd.merge(stocks,week48,on=['336','Tickers'])\n",
    "week48.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week49 = week48.rename(columns={343:'343'})\n",
    "stocks = stocks.rename(columns={'Week48':'Week49','336':'343'})\n",
    "week49 = pd.merge(stocks,week49,on=['343','Tickers'])\n",
    "week49.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week50 = week49.rename(columns={350:'350'})\n",
    "stocks = stocks.rename(columns={'Week49':'Week50','343':'350'})\n",
    "week50 = pd.merge(stocks,week50,on=['350','Tickers'])\n",
    "week50.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week51 = week50.rename(columns={357:'357'})\n",
    "stocks = stocks.rename(columns={'Week50':'Week51','350':'357'})\n",
    "week51 = pd.merge(stocks,week51,on=['357','Tickers'])\n",
    "week51.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "week52 = week51.rename(columns={364:'364'})\n",
    "stocks = stocks.rename(columns={'Week51':'Week52','357':'364'})\n",
    "week52 = pd.merge(stocks,week52,on=['364','Tickers'])\n",
    "week52.drop_duplicates(subset='Link',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reordering columns for readability \n",
    "\n",
    "final = week52[['Authors','Link','Title','Strategy','Tickers', 'Week1',\n",
    "        'Week2','Week3','Week4','Week5','Week6','Week7','Week8','Week9','Week10','Week11',\n",
    "       'Week12','Week13','Week14','Week15','Week16','Week17','Week18', 'Week19','Week20',\n",
    "       'Week21','Week22','Week23','Week24','Week25','Week26','Week27','Week28','Week29',\n",
    "       'Week30', 'Week31','Week32','Week33','Week34','Week35','Week36','Week37','Week38',\n",
    "       'Week39','Week40','Week41','Week42','Week43', 'Week44', 'Week45','Week46','Week47',\n",
    "       'Week48','Week49','Week50','Week51','Week52']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Tickers</th>\n",
       "      <th>Week1</th>\n",
       "      <th>Week2</th>\n",
       "      <th>Week3</th>\n",
       "      <th>Week4</th>\n",
       "      <th>Week5</th>\n",
       "      <th>...</th>\n",
       "      <th>Week43</th>\n",
       "      <th>Week44</th>\n",
       "      <th>Week45</th>\n",
       "      <th>Week46</th>\n",
       "      <th>Week47</th>\n",
       "      <th>Week48</th>\n",
       "      <th>Week49</th>\n",
       "      <th>Week50</th>\n",
       "      <th>Week51</th>\n",
       "      <th>Week52</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>/article/4038275-apple-unexpected-positive-app...</td>\n",
       "      <td>Apple: An Unexpected Positive Appears</td>\n",
       "      <td>Long</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.080002</td>\n",
       "      <td>121.629997</td>\n",
       "      <td>130.289993</td>\n",
       "      <td>133.289993</td>\n",
       "      <td>...</td>\n",
       "      <td>174.25</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>169.979996</td>\n",
       "      <td>174.089996</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>172.669998</td>\n",
       "      <td>176.419998</td>\n",
       "      <td>170.570007</td>\n",
       "      <td>116.150002</td>\n",
       "      <td>118.989998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Authors                                               Link  \\\n",
       "0   Paulo Santos  /article/4038275-apple-unexpected-positive-app...   \n",
       "\n",
       "                                   Title Strategy Tickers  Week1       Week2  \\\n",
       "0  Apple: An Unexpected Positive Appears     Long    AAPL  120.0  120.080002   \n",
       "\n",
       "        Week3       Week4       Week5     ...      Week43      Week44  \\\n",
       "0  121.629997  130.289993  133.289993     ...      174.25  173.970001   \n",
       "\n",
       "       Week45      Week46      Week47      Week48      Week49      Week50  \\\n",
       "0  169.979996  174.089996  169.800003  172.669998  176.419998  170.570007   \n",
       "\n",
       "       Week51      Week52  \n",
       "0  116.150002  118.989998  \n",
       "\n",
       "[1 rows x 57 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Accounting For Actual Performace \n",
    "_In order to properly interpret, analyze and model stock recommendations with historical stock pricing timeseries functionality, we must convert datatypes in order to merge the dataframes._\n",
    "\n",
    "---\n",
    "- Performance after 1 month, 3 months, 6 months, and 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final2 = final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "final['1 Month Hold'] = final['Week4'] - final['Week1']\n",
    "final['3 Month Hold'] = final['Week12'] - final['Week1']\n",
    "final['6 Month Hold'] = final['Week24'] - final['Week1']\n",
    "final['12 Month Hold'] = final['Week52'] - final['Week1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/laurencable/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Creating column represening difference in pricing 3 months after opening\n",
    "final['1 Month Hold'] = final['Week4'] - final['Week1']\n",
    "final['3 Month Hold'] = final['Week12'] - final['Week1']\n",
    "final['6 Month Hold'] = final['Week24'] - final['Week1']\n",
    "final['12 Month Hold'] = final['Week52'] - final['Week1']\n",
    "\n",
    "# If increased, apply 0\n",
    "# If decreased, apply 1\n",
    "final['1 Month Hold'] = final['1 Month Hold'].apply(lambda x: 0 if x > 0.0 else 1)\n",
    "final['3 Month Hold'] = final['3 Month Hold'].apply(lambda x: 0 if x > 0.0 else 1)\n",
    "final['6 Month Hold'] = final['6 Month Hold'].apply(lambda x: 0 if x > 0.0 else 1)\n",
    "final['12 Month Hold'] = final['12 Month Hold'].apply(lambda x: 0 if x > 0.0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = final[['Link','Strategy','Tickers','1 Month Hold','3 Month Hold','6 Month Hold','12 Month Hold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final = final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final.to_csv('tuesday.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Tickers</th>\n",
       "      <th>1 Month Hold</th>\n",
       "      <th>3 Month Hold</th>\n",
       "      <th>6 Month Hold</th>\n",
       "      <th>12 Month Hold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/article/4038275-apple-unexpected-positive-app...</td>\n",
       "      <td>Long</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Link Strategy Tickers  \\\n",
       "0  /article/4038275-apple-unexpected-positive-app...     Long    AAPL   \n",
       "\n",
       "   1 Month Hold  3 Month Hold  6 Month Hold  12 Month Hold  \n",
       "0             0             0             0              1  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving Results \n",
    "\n",
    "---\n",
    "\n",
    "#### Exporting as CSV  \n",
    "- Save `Final` `DataFrame` csv file\n",
    "- Read csv back in to ensure export was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exporting as csv\n",
    "week52.to_csv('final_dataframe.csv',index=False)\n",
    "\n",
    "# Reading csv back in \n",
    "week52 = pd.read_csv('final_dataframe.csv')\n",
    "\n",
    "# Inspecting head\n",
    "final_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onwards!\n",
    "\n",
    "---\n",
    "\n",
    "## Please proceed to Notebook 6 :)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

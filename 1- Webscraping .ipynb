{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"seekingalpha.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Notebook 1- Webscraping Seeking Alpha\n",
    "\n",
    "### _Webscraping Long and Short Ideas on Seeking Alpha_\n",
    "\n",
    "---\n",
    "## Notebook Summary\n",
    "- Utilizing the Requests and Beautiful Soup libraries to construct webscrapers and engineer functions in order to acquire the following data for Long and Short stock ideas on Seeking Alpha : \n",
    "> - \"Title\" of the article \n",
    "> - \"Author\" of the article\n",
    "> - Stock \"Ticker\" the article references \n",
    "> - Length of \"Time: article has been posted on Seeking Alpha\n",
    "> - \"Link\" of the article itself\n",
    "  The method for acquiring data will be scraping the \"Long Ideas\" and \"Short Ideas\" threads as listed under Seeking Alpha's annalyses tab from 1/1/2017 through 1/31/2018\n",
    "- Leveraging the Pandas library to transform scraped results into a DataFrame that will be exported as a csv file to be used inn subsequent notebooks.\n",
    "---\n",
    "### _Importing Necessary Libraries_\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Webscrapers \n",
    "---\n",
    "### _Long Ideas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(https://seekingalpha.com/stock-ideas/long-ideas?page=210)\n",
      "210\n",
      "200\n",
      "(https://seekingalpha.com/stock-ideas/long-ideas?page=211)\n",
      "211\n",
      "200\n",
      "(https://seekingalpha.com/stock-ideas/long-ideas?page=212)\n",
      "212\n"
     ]
    }
   ],
   "source": [
    "long_request_list = []  # Creating empty list to append results to \n",
    "\n",
    "for i in range(210,213):  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    # Pages selected to scrape selected dates \n",
    "    response = requests.get(\"https://seekingalpha.com/stock-ideas/long-ideas?page=\"+str(i), \n",
    "    headers = {'User-agent': '-----lauren'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Introducing header and delay between each request- polite web scraping practices \n",
    "    time.sleep(1)  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scraping/parsing HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        long_request_list.append(soup)   # Appending results to empty list\n",
    "    print(f'(https://seekingalpha.com/stock-ideas/long-ideas?page={str(i)})')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Short Ideas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(https://seekingalpha.com/stock-ideas/short-ideas?page=21)\n",
      "21\n",
      "200\n",
      "(https://seekingalpha.com/stock-ideas/short-ideas?page=22)\n",
      "22\n",
      "200\n",
      "(https://seekingalpha.com/stock-ideas/short-ideas?page=23)\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "short_request_list = []  # Creating empty list to append soup results to \n",
    "\n",
    "for i in range(21,24):  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    # Pages selected to scrape selected dates \n",
    "    response = requests.get(\"https://seekingalpha.com/stock-ideas/short-ideas?page=\"+str(i),\n",
    "    headers = {'User-agent': 'LaurenCable-GeneralAssemblyCapstone-6787994961-laurencable10@gmail.com'})\n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Introducing header and delay between each request- polite web scraping practices \n",
    "    time.sleep(1)     \n",
    "    print(response.status_code)  \n",
    "    if response.status_code == 200: # Scraping/parsing HTML only if proper connection made \n",
    "        html = response.text         \n",
    "        soup = BeautifulSoup(html,'lxml')  \n",
    "        short_request_list.append(soup)   # Appending results to empty list\n",
    "    print(f'(https://seekingalpha.com/stock-ideas/short-ideas?page={str(i)})')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting Information for Each Article \n",
    "---\n",
    "### Long Ideas\n",
    "### _Extracing All TItles_ \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longideas_titles(long_request_list):\n",
    "    longideas_titles = []   # Creating empty list to append article titles to \n",
    "    for i in long_request_list:  # Iterating through scraped results \n",
    "        for each in i.find_all('a',{'class':'a-title'}): # Calling proper HTML tags/classes \n",
    "            longideas_titles.append(each.text)  # Appending text data to empty list\n",
    "    return(longideas_titles)\n",
    "\n",
    "# Setting function as variable\n",
    "fetch_longideas_titles = extract_longideas_titles(long_request_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Extracing All Links_ \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_longideas_links(long_request_list):\n",
    "    longideas_links = []    # Creating empty list to append article links to \n",
    "    for i in long_request_list:  # Iterating through scraped results \n",
    "        for each in i.find_all('div',{'class':'media-body'}): # Calling proper HTML tags/classes\n",
    "            result = each.find('a')\n",
    "            if result:\n",
    "                link = result['href']\n",
    "                longideas_links.append(link) # Appending data to empty list\n",
    "    return(longideas_links)\n",
    "\n",
    "# Setting function as variable\n",
    "fetch_longideas_links = extract_longideas_links(long_request_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Extracting All Authors, Tickers, and TIme of Posting_ \n",
    "----\n",
    "- The HTML format for these three features is a little tricky, as they are separated by bullet points. However, this can be side-stepped by utilizing the bullet points as a delimiter and indexing to grab each separate feature, shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CTSH• May 7, 2017, 4:21 PM • Activist Stocks•5\\xa0Comments',\n",
       " 'GIS• May 7, 2017, 3:42 PM • D.M. Martins Research•29\\xa0Comments',\n",
       " \"Editors' Pick • MMM• May 7, 2017, 1:58 PM • Axia Enantio•8\\xa0Comments\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_longideas_multi(long_request_list):\n",
    "    longideas_multi = [] # Creating empty list to append requests to\n",
    "    for i in long_request_list: # Iterating through requests\n",
    "        for each in i.find_all('div',{'class':'a-info'}): # Calling proper HTML tags/classes\n",
    "            results = each.find_all('span')\n",
    "            longideas_multi.append(each.text) # Appending text data to empty list\n",
    "    return(longideas_multi)\n",
    "\n",
    "# Setting function as variable \n",
    "fetch_longideas_multi = extract_longideas_multi(long_request_list)\n",
    "\n",
    "# Text for all 3 features in 1 string, separated by bullets\n",
    "fetch_longideas_multi[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to separate attribute in the string above\n",
    "# Transforming results into DataFrame \n",
    "\n",
    "longideas_df = pd.DataFrame({'Multi':fetch_longideas_multi})\n",
    "longideas_df.head(3)\n",
    "\n",
    "# Utilizing bullet as delimiter\n",
    "# str-split and extract each attribute via indexing position\n",
    "longideas_tickers = longideas_df['Multi'].str.split('•').str.get(0)\n",
    "longideas_times = longideas_df['Multi'].str.split('•').str.get(1)\n",
    "longideas_authors = longideas_df['Multi'].str.split('•').str.get(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Ideas\n",
    "### _Extracing All TItles_ \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shortideas_titles(short_request_list):  \n",
    "    shortideas_titles = []  # Creating empty list to append article titles to\n",
    "    for i in short_request_list:  # Iterating through scraped results \n",
    "        for each in i.find_all('a',{'class':'a-title'}): # Calling proper HTML tags/classes   \n",
    "                shortideas_titles.append(each.text) # Appending text data to empty list \n",
    "    return(shortideas_titles)\n",
    "\n",
    "# Setting function as variable\n",
    "fetch_shortideas_titles = extract_shortideas_titles(short_request_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Extracing All Links_ \n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shortideas_links(short_request_list):\n",
    "    shortideas_links = []   # Creating empty list to append article links to\n",
    "    for i in short_request_list:  # Iterating through scraped results\n",
    "        for each in i.find_all('div',{'class':'media-body'}): # Calling proper HTML tags/classes \n",
    "            result = each.find('a')\n",
    "            if result:\n",
    "                link = result['href']\n",
    "                shortideas_links.append(link) # Appending data to empty list \n",
    "    return(shortideas_links)\n",
    "\n",
    "# Setting function as variable\n",
    "fetch_shortideas_links = extract_shortideas_links(short_request_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Extracting All Authors, Tickers, and TIme of Posting_\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HPE• Jun. 6, 2017, 1:09 PM • Jay Wei•14\\xa0Comments',\n",
       " \"Editors' Pick • MD• Jun. 6, 2017, 11:20 AM • Michael Boyd•7\\xa0Comments\",\n",
       " 'MNK, ESRX• Jun. 6, 2017, 9:36 AM • Citron Research•6\\xa0Comments']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_shortideas_multi(short_request_list):\n",
    "    shortideas_multi = [] # Creating empty list to append requests to\n",
    "    for i in short_request_list: # Iterating through requests\n",
    "        for each in i.find_all('div',{'class':'a-info'}): # Calling proper HTML tags/classes\n",
    "            results = each.find_all('span')\n",
    "            shortideas_multi.append(each.text) # Appending text data to empty list\n",
    "    return(shortideas_multi)\n",
    "\n",
    "# Set function as variable \n",
    "fetch_shortideas_multi = extract_shortideas_multi(short_request_list)\n",
    "\n",
    "# Text for all 3 features in 1 string, separated by bullets\n",
    "fetch_shortideas_multi[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to separate attribute in the string above\n",
    "# Transforming results into DataFrame \n",
    "\n",
    "shortideas_df = pd.DataFrame({'Multi':fetch_shortideas_multi})\n",
    "shortideas_df.head(3)\n",
    "\n",
    "# Utilizing bullet as delimiter\n",
    "# str-split and extract each attribute via indexing position\n",
    "\n",
    "shortideas_tickers = shortideas_df['Multi'].str.split('•').str.get(0)\n",
    "shortideas_times = shortideas_df['Multi'].str.split('•').str.get(1)\n",
    "shortideas_authors = shortideas_df['Multi'].str.split('•').str.get(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Transforming Results into DataFrames\n",
    "---\n",
    "### Long Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function results become columns for dataframe \n",
    "\n",
    "longideas = pd.DataFrame({'Time': longideas_times,'Link': fetch_longideas_links,\n",
    "                          'Title':fetch_longideas_titles,'Authors': longideas_authors,\n",
    "                          'Tickers': longideas_tickers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "longideas['Strategy'] = 'Long' # Column added for strategy type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "longideas.to_csv('long11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('long1.csv')\n",
    "b = pd.read_csv('long2.csv')\n",
    "c = pd.read_csv('long3.csv')\n",
    "d = pd.read_csv('long4.csv')\n",
    "e = pd.read_csv('long5.csv')\n",
    "f = pd.read_csv('long6.csv')\n",
    "g = pd.read_csv('long7.csv')\n",
    "h = pd.read_csv('long8.csv')\n",
    "i = pd.read_csv('long9.csv')\n",
    "j = pd.read_csv('long10.csv')\n",
    "k = pd.read_csv('long11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "longideas = pd.concat([a,b,c,d,e,f,g,h,i,j,k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "longideas.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function results become columns for dataframe \n",
    "\n",
    "shortideas = pd.DataFrame({'Time': shortideas_times,'Link': fetch_shortideas_links,\n",
    "                          'Title': fetch_shortideas_titles,'Authors': shortideas_authors,\n",
    "                           'Tickers': shortideas_tickers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortideas['Strategy'] = 'Short' # Column added for strategy type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comping \"Ideas\" DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideas = pd.concat([longideas,shortideas])  # Merging long and short ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Results - Exporting \"Ideas\" DataFrame as CSV \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Ideas dataframe as csv\n",
    "ideas.to_csv('IDEAS.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onwards\n",
    "\n",
    "## Please proceed to Notebook 2 :)  \n",
    "----\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

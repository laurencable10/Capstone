{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"seekingalpha.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Notebook 6- Webscraping Article Content \n",
    "\n",
    "_Grabbing All Text From Articles_\n",
    "\n",
    "---\n",
    "### Notebook Summary\n",
    " \n",
    "#### 1. Subsetting DataFrame\n",
    " - Due to lack of computational and amount of text scraped \n",
    "     - Almost 3,000 articles\n",
    "     - Multiple paragraphs each article\n",
    "\n",
    "#### 2. Webscraping Article Content\n",
    " - `Webscraping` selected `Seekinng Alpha` articles for their content\n",
    " - Engineered functions to extract all text from each\n",
    "----\n",
    "### _Importing Necessary Libraries_\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Importing \"Final\" DataFrame_ \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Tickers</th>\n",
       "      <th>Opening Price</th>\n",
       "      <th>Week 1</th>\n",
       "      <th>Week 2</th>\n",
       "      <th>Week 3</th>\n",
       "      <th>...</th>\n",
       "      <th>Week 42</th>\n",
       "      <th>Week 43</th>\n",
       "      <th>Week 44</th>\n",
       "      <th>Week 45</th>\n",
       "      <th>Week 46</th>\n",
       "      <th>Week 47</th>\n",
       "      <th>Week 48</th>\n",
       "      <th>Week 49</th>\n",
       "      <th>Week 50</th>\n",
       "      <th>Week 51</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Paulo Santos</td>\n",
       "      <td>/article/4038275-apple-unexpected-positive-app...</td>\n",
       "      <td>Apple: An Unexpected Positive Appears</td>\n",
       "      <td>Long</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.080002</td>\n",
       "      <td>121.629997</td>\n",
       "      <td>130.289993</td>\n",
       "      <td>...</td>\n",
       "      <td>174.25</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>169.979996</td>\n",
       "      <td>174.089996</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>172.669998</td>\n",
       "      <td>176.419998</td>\n",
       "      <td>170.570007</td>\n",
       "      <td>116.150002</td>\n",
       "      <td>118.989998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mark Hibben</td>\n",
       "      <td>/article/4038269-apple-renewing-mac-focus-new-...</td>\n",
       "      <td>Apple: Renewing Mac Focus In The New Year</td>\n",
       "      <td>Long</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.080002</td>\n",
       "      <td>121.629997</td>\n",
       "      <td>130.289993</td>\n",
       "      <td>...</td>\n",
       "      <td>174.25</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>169.979996</td>\n",
       "      <td>174.089996</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>172.669998</td>\n",
       "      <td>176.419998</td>\n",
       "      <td>170.570007</td>\n",
       "      <td>116.150002</td>\n",
       "      <td>118.989998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Rinse Terpstra</td>\n",
       "      <td>/article/4037413-apple-user-base</td>\n",
       "      <td>Apple: It's All About The User Base</td>\n",
       "      <td>Long</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.080002</td>\n",
       "      <td>121.629997</td>\n",
       "      <td>130.289993</td>\n",
       "      <td>...</td>\n",
       "      <td>174.25</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>169.979996</td>\n",
       "      <td>174.089996</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>172.669998</td>\n",
       "      <td>176.419998</td>\n",
       "      <td>170.570007</td>\n",
       "      <td>116.150002</td>\n",
       "      <td>118.989998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0          Authors  \\\n",
       "0           0     Paulo Santos   \n",
       "1           1      Mark Hibben   \n",
       "2           2   Rinse Terpstra   \n",
       "\n",
       "                                                Link  \\\n",
       "0  /article/4038275-apple-unexpected-positive-app...   \n",
       "1  /article/4038269-apple-renewing-mac-focus-new-...   \n",
       "2                   /article/4037413-apple-user-base   \n",
       "\n",
       "                                       Title Strategy Tickers  Opening Price  \\\n",
       "0      Apple: An Unexpected Positive Appears     Long    AAPL          120.0   \n",
       "1  Apple: Renewing Mac Focus In The New Year     Long    AAPL          120.0   \n",
       "2        Apple: It's All About The User Base     Long    AAPL          120.0   \n",
       "\n",
       "       Week 1      Week 2      Week 3     ...      Week 42     Week 43  \\\n",
       "0  120.080002  121.629997  130.289993     ...       174.25  173.970001   \n",
       "1  120.080002  121.629997  130.289993     ...       174.25  173.970001   \n",
       "2  120.080002  121.629997  130.289993     ...       174.25  173.970001   \n",
       "\n",
       "      Week 44     Week 45     Week 46     Week 47     Week 48     Week 49  \\\n",
       "0  169.979996  174.089996  169.800003  172.669998  176.419998  170.570007   \n",
       "1  169.979996  174.089996  169.800003  172.669998  176.419998  170.570007   \n",
       "2  169.979996  174.089996  169.800003  172.669998  176.419998  170.570007   \n",
       "\n",
       "      Week 50     Week 51  \n",
       "0  116.150002  118.989998  \n",
       "1  116.150002  118.989998  \n",
       "2  116.150002  118.989998  \n",
       "\n",
       "[3 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.head(3) # Printing first few rows of data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Compiling List of Links to Gather Article Content for_ \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = final['Link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/article/4038275-apple-unexpected-positive-appears'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final['Link'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get('https://seekingalpha.com/article/4184106-gulfport-energy-special-situation-conviction-strong-buy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []  # Creating empty list to append soup results to \n",
    "\n",
    "for link in links:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    # Pages selected to scrape selected dates \n",
    "    response = requests.get(\"https://seekingalpha.com\"+ str(link),\n",
    "    headers = {'User-agent': 'LaurenCable-GeneralAssemblyCapstone-6787994961-laurencable10@gmail.com'})\n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Introducing header and delay between each request- polite web scraping practices \n",
    "    time.sleep(30)     \n",
    "    print(response.status_code)  \n",
    "    if response.status_code == 200: # Scraping/parsing HTML only if proper connection made \n",
    "        html = response.text         \n",
    "        soup = BeautifulSoup(html,'lxml')  \n",
    "        short_request_list.append(soup)   # Appending results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separarting dataframe by strategy type\n",
    "long = final_dataframe[final_dataframe['Strategy']=='Long']\n",
    "short = final_dataframe[final_dataframe['Strategy']=='Short']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separarting dataframe by strategy type\n",
    "long = final[final['Strategy']=='Long']\n",
    "short = final[final['Strategy']=='Short']\n",
    "\n",
    "# Unique tickers only in each strategy type dataframe\n",
    "long_tickers = long.drop_duplicates('Tickers')\n",
    "short_tickers = short.drop_duplicates('Tickers')\n",
    "\n",
    "# Compiling lists of unique tickers\n",
    "long_links = long_tickers['Link'].tolist()\n",
    "short_links = short_tickers['Link'].tolist()\n",
    "\n",
    "# Combining lists\n",
    "language_links = long_links + short_links\n",
    "\n",
    "# Subsetting final dataframe on list\n",
    "language_dataframe = final[final['Link'].isin(language_links)]\n",
    "language_dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe['Link'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Constructing Webscrapers \n",
    "---\n",
    "### _Links_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_request_list = []  # Creating empty list to append soup results to \n",
    "\n",
    "for i in range(5,28):  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    # Pages selected to scrape selected dates \n",
    "    response = requests.get(\"https://seekingalpha.com/stock-ideas/short-ideas?page=\"+str(i),\n",
    "    headers = {'User-agent': 'LaurenCable-GeneralAssemblyCapstone-6787994961-laurencable10@gmail.com'})\n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Introducing header and delay between each request- polite web scraping practices \n",
    "    time.sleep(30)     \n",
    "    print(response.status_code)  \n",
    "    if response.status_code == 200: # Scraping/parsing HTML only if proper connection made \n",
    "        html = response.text         \n",
    "        soup = BeautifulSoup(html,'lxml')  \n",
    "        short_request_list.append(soup)   # Appending results to empty list\n",
    "    print(f'(https://seekingalpha.com/stock-ideas/short-ideas?page={str(i)})')\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "\n",
    "    \n",
    "response = requests.get('https://seekingalpha.com/article/4037958-tesla-believe-morgan-stanley',\n",
    "           headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'})\n",
    "html = response.text\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "content.append(soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Text & Transforming into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating function to extract article text\n",
    "def extract_article_text(request_list): \n",
    "    articles = []   # Empty list to append article text to \n",
    "    for i in request_list:  # Iterating through scraped results \n",
    "        for each in i.find_all('div',{'id':'a-body'}): # Calling proper HTML tags/classes \n",
    "            articles.append(each.text)  # Appending text data to empty list\n",
    "    return(articles)\n",
    "\n",
    "# Setting function as variable\n",
    "fetch_articles = extract_article_text(request_list_14)\n",
    "\n",
    "# Creating dataframes with fetched results as column (total- 30 dataframes)\n",
    "text = pd.DataFrame(fetch_articles)\n",
    "\n",
    "# Exporting as csv\n",
    "text.to_csv('articles14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Final DataFrame\n",
    "---\n",
    "#### Appending Article Content to Language DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = pd.read_csv('article_content.csv')  # Reading article content csv \n",
    "\n",
    "article_content['0'] = language_dataframe['Article Content'] # Transforming into column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe.head(2)  # Inspecting progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving Results \n",
    "\n",
    "---\n",
    "\n",
    "#### Exporting as CSV  \n",
    "- Save `Language` `DataFrame` as csv file  \n",
    "- Read csv back in to ensure export was successful\n",
    "- Will undergo `Natural` `Language` `Processing` and `Topic` `Modelling` in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_dataframe.to_csv('language_dataframe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

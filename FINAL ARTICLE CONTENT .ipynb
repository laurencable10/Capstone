{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"seekingalpha.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Notebook 6- Compiling Article Content \n",
    "\n",
    "_Grabbing All Text From Articles_\n",
    "\n",
    "---\n",
    "### Notebook Summary\n",
    " \n",
    "- Code to `Webscrape` both the long and short analyses (articles) on Seeking Alpha  \n",
    "\n",
    "- `Engineered` `Functions` to extract all article text \n",
    "\n",
    "- Light data munging in order to properly assemble a final `Ideas` `DataFrame` to be used in subsequent notebooks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"seekingalpha.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Notebook 6- Compiling Article Content \n",
    "\n",
    "_Grabbing All Text From Articles_\n",
    "\n",
    "---\n",
    "### Notebook Summary\n",
    " \n",
    "#### 1. Subsetting DataFrame\n",
    " - Due to lack of computational and amount of text scraped \n",
    "     - Almost 3,000 articles\n",
    "     - Multiple paragraphs each article\n",
    "\n",
    "#### 2. Webscraping Article Content\n",
    " - `Webscraping` selected `Seekinng Alpha` articles for their content\n",
    " - Engineered functions to extract all text from each\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import socks\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Reading in Final DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "      <th>Strategy</th>\n",
       "      <th>Tickers</th>\n",
       "      <th>Date</th>\n",
       "      <th>Opening Price</th>\n",
       "      <th>Week1</th>\n",
       "      <th>Week2</th>\n",
       "      <th>Week3</th>\n",
       "      <th>...</th>\n",
       "      <th>Week44</th>\n",
       "      <th>Week45</th>\n",
       "      <th>Week46</th>\n",
       "      <th>Week47</th>\n",
       "      <th>Week48</th>\n",
       "      <th>Week49</th>\n",
       "      <th>Week50</th>\n",
       "      <th>Week51</th>\n",
       "      <th>Week52</th>\n",
       "      <th>Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mark Hibben</td>\n",
       "      <td>/article/4034776-apple-oddly-satisfying-touchs...</td>\n",
       "      <td>Apple: The 'Oddly Satisfying' Touchscreen Mac</td>\n",
       "      <td>Long</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>116.150002</td>\n",
       "      <td>118.989998</td>\n",
       "      <td>120.0</td>\n",
       "      <td>120.080002</td>\n",
       "      <td>...</td>\n",
       "      <td>174.25</td>\n",
       "      <td>173.970001</td>\n",
       "      <td>169.979996</td>\n",
       "      <td>174.089996</td>\n",
       "      <td>169.800003</td>\n",
       "      <td>172.669998</td>\n",
       "      <td>176.419998</td>\n",
       "      <td>170.570007</td>\n",
       "      <td>116.150002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Authors                                               Link  \\\n",
       "0   Mark Hibben  /article/4034776-apple-oddly-satisfying-touchs...   \n",
       "\n",
       "                                           Title Strategy Tickers        Date  \\\n",
       "0  Apple: The 'Oddly Satisfying' Touchscreen Mac     Long    AAPL  2017-01-03   \n",
       "\n",
       "   Opening Price       Week1  Week2       Week3     ...       Week44  \\\n",
       "0     116.150002  118.989998  120.0  120.080002     ...       174.25   \n",
       "\n",
       "       Week45      Week46      Week47      Week48      Week49      Week50  \\\n",
       "0  173.970001  169.979996  174.089996  169.800003  172.669998  176.419998   \n",
       "\n",
       "       Week51      Week52  Performance  \n",
       "0  170.570007  116.150002            0  \n",
       "\n",
       "[1 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataframe= pd.read_csv('final_dataframe.csv')\n",
    "final_dataframe.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merging the stock and idea datframes on a weekly basis utilizing pandas datetime funclen(final_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Subsetting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separarting dataframe by strategy type\n",
    "long = final_dataframe[final_dataframe['Strategy']=='Long']\n",
    "short = final_dataframe[final_dataframe['Strategy']=='Short']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separarting dataframe by strategy type\n",
    "long = final_dataframe[final_dataframe['Strategy']=='Long']\n",
    "short = final_dataframe[final_dataframe['Strategy']=='Short']\n",
    "\n",
    "# Unique tickers only in each strategy type dataframe\n",
    "long_tickers = long.drop_duplicates('Tickers')\n",
    "short_tickers = short.drop_duplicates('Tickers')\n",
    "\n",
    "# Compiling lists of unique tickers\n",
    "long_links = long_tickers['Link'].tolist()\n",
    "short_links = short_tickers['Link'].tolist()\n",
    "\n",
    "# Combining lists\n",
    "language_links = long_links + short_links\n",
    "\n",
    "# Subsetting final dataframe on list\n",
    "language_dataframe = final_dataframe[final_dataframe['Link'].isin(language_links)]\n",
    "language_dataframe.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Gathering Article Content \n",
    "\n",
    "---\n",
    "\n",
    "#### Constructing Webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = final_dataframe['Link'].tolist() # List of links to webscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "/article/4075399-apples-tencent-run\n",
      "200\n",
      "/article/4075351-apple-berkshire-hathaway-following-buffett-buffett-two-different-things\n",
      "200\n",
      "/article/4077894-apples-wwdc-expected-reveal-new-hardware\n",
      "200\n",
      "/article/4077360-can-apple-end-wwdc-losing-streak\n",
      "200\n",
      "/article/4078582-apple-services-unlikely-offset-soft-iphone-sales\n",
      "200\n",
      "/article/4079737-make-money-apple-skeptics\n",
      "200\n",
      "/article/4079728-apple-bank\n",
      "200\n",
      "/article/4079698-apple-business-chat-imessage-threatens-facebooks-messenger-business-paypal\n",
      "200\n",
      "/article/4079671-investors-sell-apple\n",
      "200\n",
      "/article/4079379-apple-analysts-react-homepod\n"
     ]
    }
   ],
   "source": [
    "request_list_13 = []\n",
    "for link in links[120:130]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'aaaaaa,,,,cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(2)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_13.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "/article/4079319-overlooked-surprises-apples-wwdc-keynote\n",
      "200\n",
      "/article/4079212-apple-key-wwdc-takeaways\n",
      "200\n",
      "/article/4079016-apples-wwdc-hardware-os-updates-plus-killer-ar\n",
      "200\n",
      "/article/4082207-assessing-apples-prospects-india\n",
      "200\n",
      "/article/4082127-apple-breaks-bad-streaming-video\n",
      "200\n",
      "/article/4081648-can-apple-disrupt-automotive-manufacturing\n",
      "200\n",
      "/article/4081514-official-apple-getting-cars\n",
      "200\n",
      "/article/4081216-apple-laughs-googles-pixel-flop\n",
      "200\n",
      "/article/4081108-apple-cash-easy-buy-disney\n",
      "200\n",
      "/article/4081092-apple-comes-next-months\n"
     ]
    }
   ],
   "source": [
    "request_list_14 = []\n",
    "for link in links[130:140]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'laurencable-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(2)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_14.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "/article/4080915-apples-selloff-justified-dcf-analysis\n",
      "200\n",
      "/article/4080736-apple-well\n",
      "200\n",
      "/article/4082087-little-likely-late-apples-next-iphone\n",
      "200\n",
      "/article/4083631-apple-growth\n",
      "200\n",
      "/article/4083423-idc-sees-wearables-market-nearly-doubling-2021\n",
      "200\n",
      "/article/4083245-10-reasons-buy-apple-now\n",
      "200\n",
      "/article/4082803-apples-war-leaks-gets-results\n",
      "200\n",
      "/article/4082340-apple-time-go-low\n",
      "200\n",
      "/article/4085182-going-convince-hold-outs-upgrade-apples-iphone-8\n",
      "200\n",
      "/article/4084647-apples-ai-gap-1-year-later\n"
     ]
    }
   ],
   "source": [
    "request_list_15 = []\n",
    "for link in links[140:150]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'hhhhh[[[[[[cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(2)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_15.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "/article/4084572-apple-augmented-reality-money\n",
      "200\n",
      "/article/4084359-apple-beginners\n",
      "429\n",
      "/article/4083933-apple-buy-dollars-less-70-cents\n",
      "200\n",
      "/article/4083926-augmented-reality-apples-revolutionary-offering\n",
      "200\n",
      "/article/4084563-apples-price-chart-entering-ugly-territory\n",
      "200\n",
      "/article/4086389-options-apple-offer-leveraged-profit-opportunity\n",
      "200\n",
      "/article/4086358-apple-140-bottom-buy\n",
      "200\n",
      "/article/4085961-apples-fastest-processor-ever\n",
      "200\n",
      "/article/4085871-apple-3-different-valuation-measures-lead-conclusion\n",
      "403\n",
      "/article/4086920-apples-iphone-8-dilemma\n"
     ]
    }
   ],
   "source": [
    "request_list_16 = []\n",
    "for link in links[150:160]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'caaaaaablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(2)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_16.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "request_list_9 = []\n",
    "for link in links[80:90]:  \n",
    "    # Range (x,x) indicates pages being scraped\n",
    "    response = requests.get('https://seekingalpha.com'+(link), \n",
    "    headers = {'User-agent': 'cablegirl-dsi-ga'}) \n",
    "        # 'str(i)' indicates page number being scraped  \n",
    "        #  Header- way for website owners to contact you   \n",
    "    time.sleep(5)  # 30 second delay between each request  \n",
    "    print(response.status_code) \n",
    "    if response.status_code == 200: # Scrape/parse HTML only if proper connection made \n",
    "        html = response.text    \n",
    "        soup = BeautifulSoup(html,'lxml') \n",
    "        request_list_9.append(soup)   # Append results to empty list\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Text & Transforming into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating function to extract article text\n",
    "def extract_article_text(request_list): \n",
    "    articles = []   # Empty list to append article text to \n",
    "    for i in request_list:  # Iterating through scraped results \n",
    "        for each in i.find_all('div',{'id':'a-body'}): # Calling proper HTML tags/classes \n",
    "            articles.append(each.text)  # Appending text data to empty list\n",
    "    return(articles)\n",
    "\n",
    "# Setting function as variable\n",
    "fetch_articles = extract_article_text(request_list_14)\n",
    "\n",
    "# Creating dataframes with fetched results as column (total- 30 dataframes)\n",
    "text = pd.DataFrame(fetch_articles)\n",
    "\n",
    "# Exporting as csv\n",
    "text.to_csv('articles14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Final DataFrame\n",
    "---\n",
    "#### Appending Article Content to Language DataFrame\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_content = pd.read_csv('article_content.csv')  # Reading article content csv \n",
    "\n",
    "article_content['0'] = language_dataframe['Article Content'] # Transforming into column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_dataframe.head(2)  # Inspecting progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving Results \n",
    "\n",
    "---\n",
    "\n",
    "#### Exporting as CSV  \n",
    "- Save `Language` `DataFrame` as csv file  \n",
    "- Read csv back in to ensure export was successful\n",
    "- Will undergo `Natural` `Language` `Processing` and `Topic` `Modelling` in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "language_dataframe.to_csv('language_dataframe.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
